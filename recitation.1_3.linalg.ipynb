{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM E4040 2024 Fall Recitations\n",
    "\n",
    "## Session 1.3 - Linear Algebra Refresher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "* Numerical Method course (APMA4300) by Prof.Mandli:\n",
    "https://www.coursicle.com/columbia/courses/APMA/E4300/\n",
    "* Matrix Algebra - Linear Algebra for Deep Learning:\n",
    "https://www.quantstart.com/articles/matrix-algebra-linear-algebra-for-deep-learning-part-2\n",
    "* Deep Learning (book) — Ian Goodfellow, etc. Chapter 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep learning requires very good knowledge of linear algebra, and is a prerequisite for this course. This recitaion aims to help you recall some important concepts in linear algebra, and provide insights into topics which are related to ML/DL with python implementations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matrix-Vector Multiplication\n",
    "\n",
    "Multiplying a matrix by a vector can be thought of as multiplying each row of the matrix to the (column) vector. The output will be a vector that has the same number of elements as the first dimension of the matrix.\n",
    "\n",
    "For some matrix $A \\in \\mathbb{R}^{M \\times N}$ and vector $x \\in \\mathbb{R}^N$, the output $y$ is defined as\n",
    "\n",
    "$$\n",
    "y = Ax \\in \\mathbb{R}^M \\quad \\text{where} \\quad \n",
    "y_i = \\sum^N_{j=1} a_{ij} x_j \\quad \\forall i \\in \\{1, \\dots, M\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $A$ can be interpreted as a map from $\\mathbb{R}^N$ to $\\mathbb{R}^M$. You can also regard $y$ as a linear combination of the columns of $A$ where each column's weight is $x_j$, i.e.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} ~ \\\\ ~ \\\\ y \\\\ ~ \\\\ ~ \\end{bmatrix} = \n",
    "\\begin{bmatrix} ~ & ~ & ~ & ~ \\\\ ~ & ~ & ~ & ~  \\\\ a_{i1} & a_{i2} & \\cdots & a_{in} \\\\ ~ & ~ & ~ & ~  \\\\ ~ & ~ & ~ & ~ \\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = x_1 \\begin{bmatrix} ~ \\\\ ~ \\\\ a_{i1} \\\\ ~ \\\\ ~ \\end{bmatrix} + x_2 \\begin{bmatrix} ~ \\\\ ~ \\\\ a_{i2} \\\\ ~ \\\\ ~ \\end{bmatrix} + \\cdots + x_n \\begin{bmatrix} ~ \\\\ ~ \\\\ a_{in} \\\\ ~ \\\\ ~ \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This view will be useful later when we are trying to interpret various types of matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One of the most fundamental matrix-vector multiplication property is **linearity** (i.e. $A$ is a linear map).\n",
    "\n",
    "For any $x, y \\in \\mathbb{R}^N$ and any $c \\in \\mathbb{R}$, we have\n",
    "\n",
    "1. $A (x + y) = Ax + Ay$\n",
    "2. $A(cx) = c A x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matrix-Matrix Multiplication\n",
    "\n",
    "Similarly, the matrix multiplication of $A \\in \\mathbb{R}^{M \\times N}$ with another matrix $X \\in \\mathbb{R}^{N \\times D}$ is defined as\n",
    "\n",
    "$$\n",
    "Y = A X \\in \\mathbb{R}^{M \\times D} \\quad \\text{where} \\quad\n",
    "Y_{ij} = \\sum_{k=1}^N A_{ik} X_{kj}\n",
    "$$\n",
    "\n",
    "Again, $A$ can be interpreted as a map from $\\mathbb{R}^{N \\times D}$ to $\\mathbb{R}^{M \\times D}$. One may also say that the product result $Y$ is the a linear combination of the columns of $A$, with $D$ different sets of weights $X_{:,j}$.\n",
    "\n",
    "Note that you can only multiply matrices together if the number of the first matrix’s columns matches the number of the second matrix’s rows. The result will be a matrix with the same number of rows as the first matrix and the same number of columns as the second matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Matrix-Matrix multiplication has the following properties:\n",
    "1. Distributivity\n",
    "\n",
    "    $ A(B + C) = AB + AC $\n",
    "\n",
    "2. Non-Commutativity (in general)\n",
    "\n",
    "    $AB \\neq BA$\n",
    "\n",
    "3. $A(cB) = cAB$ where $c \\in \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example: Matrix Multiplication with NumPy\n",
    "\n",
    "NumPy and SciPy contain routines that are optimized to perform matrix-vector and matrix-matrix multiplication. Given two `ndarray`-s you can take their product by using the `np.matmul` function (or `@` operator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "M, N = 3, 5\n",
    "\n",
    "A = np.random.random((M, N))\n",
    "x = np.random.random(N)\n",
    "print('A @ x =', np.matmul(A, x))\n",
    "\n",
    "B = np.random.random((N, M))\n",
    "print('A @ B =\\n', A @ B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Not commutative. \n",
    "\n",
    "($AB \\neq BA$ in general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A @ B =')\n",
    "print(A @ B)\n",
    "print('B @ A =')\n",
    "print(B @ A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Product (scalar product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important special case of matrix-matrix multiplication occurs between two vectors and is known as the dot product. It has a deep relationship with geometry.\n",
    "\n",
    "The usual notation for a dot product between two $N$-dimensional vectors $x, y \\in \\mathbb{R}^N$ is\n",
    "\n",
    "$$x \\cdot y \\quad \\text{or} \\quad x^T y$$\n",
    "\n",
    "Note that $x$ and $y$ there are usually considered to be **column vectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given two vectors $x, y \\in \\mathbb{R}^n$, it is straightforward to define the **dot product** that produces a scalar value and is commutative, i.e.\n",
    "\n",
    "$$x^{T} y = \\sum_{i=1}^{N} x_{i} y_{i} = y^{T} x$$\n",
    "\n",
    "The dot product has an important geometric meaning. It is the product of the (Euclidean) *lengths* of the two vectors and the cosine of the angle $\\theta$ between them, which is written as\n",
    "\n",
    "$$x^{T} y = |x| |y| \\cos \\theta$$\n",
    "\n",
    "To find the (Euclidean) length of a vector you can simply take the square root of the dot product of the vector with itself, i.e.\n",
    "\n",
    "$$|x| = \\sqrt{x^{T} x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, the dot product is a special case of a more general mathematical entity known as an **inner product**, which is defined in vector spaces. Inner product allows intuitive concepts such as length and angle of a vector to be made rigorous in a more general setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadamard Product (element-wise multiplication)\n",
    "\n",
    "The element-wise multiplication of matrices can be calculated, but it __differs from the definition of matrix-matrix multiplication__ above. The Hadamard product of two matrices $A$ and $B$ can be denoted as\n",
    "\n",
    "$$(A \\odot B)_{i,j} = (A)_{i,j} (B)_{i,j}$$\n",
    "\n",
    "where $A$ and $B$ should both be in the same size. \n",
    "\n",
    "That is, the elements of the new matrix are simply given as the scalar multiples of each of the elements from the other matrices. Note that the Hardamard product is **commutative**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inverse\n",
    "\n",
    "A square matrix $A \\in \\mathbb{R}^{N \\times N}$ is said to have an inverse if\n",
    "\n",
    "$$\\exists ~ B \\in \\mathbb{R}^{N \\times N} \\quad \\text{s.t.} \\quad A B = B A = I$$\n",
    "\n",
    "where $I$ is the identity matrix. Then we define $B = A^{-1}$ as the inverse of $A$.\n",
    "\n",
    "If $A^{-1}$ exists, then $A$ is said to be **invertible** or **non-singular**. We have that\n",
    "\n",
    "$$AA^{-1} = A^{-1}A = I$$\n",
    "\n",
    "Basically, inverses can help us solve linear system of equations $y = A x$. If $A$ is invertible, then\n",
    "\n",
    "$$x = A^{-1} y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Solve $Ax=b$\n",
    "\n",
    "You can get the inverse of a matrix in numpy using `np.linalg.inv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.eye(N) # Return a 2-D array with ones on the diagonal and zeros elsewhere.\n",
    "A = np.random.random((N,N)) # (n x n) array of random numbers\n",
    "b = np.random.random(N) # n size array of random numbers\n",
    "\n",
    "print(\n",
    "    '\\nA^{-1}A = AA^{-1} = I ?:',\n",
    "    np.allclose(A.dot(np.linalg.inv(A)), np.linalg.inv(A).dot(A)), ',',\n",
    "    np.allclose(I, np.linalg.inv(A).dot(A))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nx = A^{-1}b:')\n",
    "x = np.linalg.inv(A).dot(b)\n",
    "print(np.linalg.inv(A).dot(b))\n",
    "print('\\nVerify: b = Ax?', np.allclose(A.dot(x), b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector Norms\n",
    "\n",
    "In mathematics, **norms** provide a general way of measuring the \"size\" of an object is space $X$.\n",
    "\n",
    "A norm is a real-valued function $\\|\\cdot\\|: X \\to R$ that satisfies:\n",
    "1. Positive definiteness\n",
    "   $$\\|x\\| \\ge 0, \\quad \\forall x \\in X$$\n",
    "2. Absolute homogeneity\n",
    "   $$\\|cx\\| = |c| \\cdot \\|x\\|, \\quad c \\in R$$\n",
    "3. Triangle inequality\n",
    "   $$\\|x_1 + x_2\\| \\le \\|x_1\\| + \\|x_2\\|, \\quad \\forall x_1, x_2 \\in X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common definition of norm is the $\\ell_p$ norm. For $x \\in \\mathbb{R}^N$ and $p \\geq 1$,\n",
    "\n",
    "$$\\|x\\|_p = (\\sum_i |x_i|^p)^{\\frac{1}{p}}$$\n",
    "\n",
    "Several typical $\\ell_p$ norms are given as follows:\n",
    "1. $\\ell_1$ norm:\n",
    "$$\n",
    "    \\|x\\|_1 = \\sum^N_{i=1} |x_i|\n",
    "$$\n",
    "2. $\\ell_2$ (Euclidean) norm:\n",
    "$$\n",
    "    \\|x\\| = \\|x\\|_2 = \\left( \\sum^N_{i=1} |x_i|^2 \\right)^{1/2}\n",
    "$$\n",
    "3. $\\ell_\\infty$ norm (supremum norm):\n",
    "$$\n",
    "    ||x||_\\infty = \\max |x_i|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that there are also other norms denoted by capital letters ($L_p$ norms). In this case we use the lower-case notation to denote finite or discrete versions of the infinite dimensional counterparts.\n",
    "\n",
    "The dot product of two vectors can be rewritten in terms of norms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Unit-ball in $\\ell_p$ Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Numpy, you can use `numpy.linalg.norm` to calculate the order of 1, 2, and $\\infty$. But here let's try to derive things from scratch! For a particular norm, let's plot the unit-ball (the set of points of distance less than or equal to 1 from a fixed central point) in $\\mathbb R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbgrader": {
     "checksum": "0a1a67dc3cba7cf47fe0f3f594d1cfcd",
     "grade": true,
     "grade_id": "cell-285505236323010",
     "locked": false,
     "points": 15,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def unit_ball(axes, ord):\n",
    "    \"\"\"Plot the unit-ball in $\\mathbb R^2$\n",
    "\n",
    "    :Input:\n",
    "     - *axes* (matplotlib.axes) Axes to plot the ball on\n",
    "     - *ord* (float) The norm requested.\n",
    "    \"\"\"\n",
    "\n",
    "    x1 = np.linspace(-1, 1, 1000) # Return evenly spaced numbers over a specified interval\n",
    "    # use the definition to calculate norm.\n",
    "    norm = (1-(np.abs(x1))**(ord))**(1/ord)\n",
    "    axes.plot(x1, norm, '-r')\n",
    "    axes.plot(x1, -norm, '-r')\n",
    "    if (ord == np.infty):\n",
    "        axes.plot([-1.0, -1.0], [-1.0, 1.0],'-r')\n",
    "        axes.plot([1.0, 1.0], [-1.0, 1.0],'-r')\n",
    "    axes.plot(0, 0, '-ob')\n",
    "    axes.set_xlim([-1.1, 1.1])\n",
    "    axes.set_ylim([-1.1, 1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(fig.get_figwidth() * 3)\n",
    "fig.set_figheight(fig.get_figheight() * 3)\n",
    "norms = [1.0 / 3.0, 0.5, 4.0 / 5.0, 1, 1.3, 1.5, 1.75, 2, 4, 9, 16, np.infty]\n",
    "titles = [\"1/3-norm\", \"1/2-norm\", \"4/5-norm\", \"1-norm\", \"4/3-norm\", \"3/2-norm\", \"7/4-norm\", \n",
    "          \"2-norm\", \"$2^2$-norm\", \"$3^2$-norm\", \"$4^2$-norm\", \"$\\infty$-norm\"]\n",
    "for (i, ord) in enumerate(norms):\n",
    "    axes = fig.add_subplot(3, 4, i + 1, aspect='equal')\n",
    "    unit_ball(axes, ord)\n",
    "    axes.set_title(titles[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite intuitive that in terms of $\\ell_1$ the unit ball is a diamond, $\\ell_2$ is a circle and $\\ell_\\infty$ is a square. With the increase of the norm order, the 4 sides of the \"unit ball\" vary from concave to convex gradually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avanced topics (depricated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In polynomial cases, we want to fit a particular function according to a given number of data points. We have $m$ data points but only want to fit a $n - 1$ order polynomial through the data where $n - 1 \\leq m$. One common approach to this problem is to minimize the \"least-squares\" error between the data $y$ and the resulting function $f(x)$:\n",
    "$$\n",
    "    E = \\left( \\sum^m_{i=1} |y_i - f(x_i)|^2 \\right )^{1/2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now if we want to solve a system of equations like\n",
    "$$\n",
    "Ax=b\n",
    "$$\n",
    "And suppose matrix $A$ is now $m \\times n$ and looks like\n",
    "$$\n",
    "    A = \\begin{bmatrix}\n",
    "        1 & x_1 & x_1^2 & \\cdots & x_1^{n-1} \\\\\n",
    "        1 & x_2 & x_2^2 & \\cdots & x_2^{n-1} \\\\\n",
    "        \\vdots & \\vdots & \\vdots & & \\vdots \\\\\n",
    "        1 & x_m & x_m^2 & \\cdots & x_m^{n-1}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Turns out if we solve the system\n",
    "\n",
    "$$A^T A x = A^T b$$\n",
    "\n",
    "we can guarantee that the error is minimized in the least-squares sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example:  Linear least squares in numpy\n",
    "\n",
    "In this case, we try to fit a line through the data points. Linear least squares can be solved in numpy using `np.linalg.lstsq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lstsq(data, x, p):\n",
    "    # YOUR CODE HERE\n",
    "    y=data[:,1]\n",
    "    y_est=np.zeros_like(x)\n",
    "    for i in range(p.shape[0]):\n",
    "        y_est += p[i] * x ** (i) \n",
    "    fig = plt.figure()\n",
    "    axes = fig.add_subplot(1, 1, 1)\n",
    "    axes.plot(data[:,0], y, 'ko')\n",
    "    axes.plot(x, y_est, 'b')\n",
    "    axes.set_title(\"Least Squares Fit to Data\")\n",
    "    axes.set_xlabel(\"$x$\")\n",
    "    axes.set_ylabel(\"$f(x)$ and $y_i$\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Linear Least Squares Problem\n",
    "N = 10\n",
    "N_p = 4\n",
    "data = np.empty((N, 2)) # Return a new array of given shape and type, without initializing entries.\n",
    "data[:, 0] = np.random.uniform(size=N)\n",
    "data[:, 0] = np.linspace(0.1, 0.9, N)\n",
    "data[:, 1] = np.sin(np.exp(-data[:, 0]**2)) + np.random.uniform(size=N)\n",
    "\n",
    "A = np.vander(data[:,0], N_p + 1)\n",
    "p = np.flipud(np.linalg.lstsq(A, data[:, 1], rcond=None)[0])\n",
    "\n",
    "# Plot result\n",
    "x = np.linspace(0.0, 1.0, 100)\n",
    "plot_lstsq(data, x, p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Decomposition mostly related to __dimensionality reduction__ of a matrix.\n",
    "\n",
    "The definition of singular value decomposition: \n",
    "\n",
    "Let $A \\in \\mathbb R^{m \\times n}$, then $A$ can be factored as\n",
    "$$\n",
    "    A = U\\Sigma V^{T}\n",
    "$$\n",
    "\n",
    "where,\n",
    "* $U \\in \\mathbb R^{m \\times m}$ and is the orthogonal matrix whose columns are the eigenvectors of $AA^{T}$\n",
    "* $V \\in \\mathbb R^{n \\times n}$ and is the orthogonal matrix whose columns are the eigenvectors of $A^{T}A$\n",
    "* $\\Sigma \\in \\mathbb R^{m \\times n}$ and is a diagonal matrix with elements $\\sigma_{1}, \\sigma_{2}, \\sigma_{3}, ... \\sigma_{r}$ where $r = rank(A)$ corresponding to the square roots of the eigenvalues of $A^{T}A$. They are called the singular values of $A$ and are non negative arranged in descending order. \n",
    "\n",
    "     ($\\sigma_{1} \\geq \\sigma_{2} \\geq \\sigma_{3} \\geq ... \\sigma_{r} \\geq 0$)\n",
    "\n",
    "NOTE: Orthogonal matrix - real square matrix whose columns and rows are orthogonal unit vectors (orthonormal vectors).\n",
    "\n",
    "One way to express this is\n",
    "$$\n",
    "{\\displaystyle Q^{\\mathrm {T} }Q=QQ^{\\mathrm {T} }=I}\n",
    "$$ where $Q^{\\mathrm {T} }$ is the transpose of $Q$ and \n",
    "$I$ is the identity matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank of a matrix is defined as (a) the maximum number of linearly independent column vectors in the matrix or (b) the maximum number of linearly independent row vectors in the matrix. Both definitions are equivalent.\n",
    "\n",
    "For an $ m × n $ matrix,\n",
    "\n",
    "If m is less than n, then the maximum rank of the matrix is m.\n",
    "If m is greater than n, then the maximum rank of the matrix is n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: SVD in numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD can be accessed using `np.linalg.svd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rect = np.array([[1,1], [3,1], [-1,4]], dtype = np.float32)\n",
    "print('Original')\n",
    "print(rect)\n",
    "U, diag, V = np.linalg.svd(rect, full_matrices=True)\n",
    "Sig = np.zeros((3,2))\n",
    "Sig[:2, :2] = np.diag(diag)\n",
    "print('Reconstructed')\n",
    "print(U.dot(Sig).dot(V.transpose()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalue Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Eigenproblems__: The general eigenproblems can be described by\n",
    "$$\n",
    "    A x = \\lambda x\n",
    "$$\n",
    "That means an __eigenvector__ of a square matrix $A$ is a nonzero vector $x$ such that multiplication by $A$ alters only the scale of $x$. The scalar, $\\lambda$, is the eigenvalue corresponding to this eigenvector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, suppose that $A$ has $n$ linearly independent eigenvectors, we let the matrix $X=\\{ x^{(1)}, \\cdots ,x^{(n)} \\}$ contain the eigenvectors of $A$ which are linearly independent, then we can write a decomposition of the matrix $A$ as\n",
    "$$\n",
    "    A = X \\Lambda X^{-1}\n",
    "$$\n",
    "where $\\Lambda = diag(\\mathbf{\\lambda}), \\mathbf{\\lambda}= \\{\\lambda_1, \\cdots, \\lambda_n\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word \"eigen\" is actually German, which means \"special\". The eigenvectors are special because they only get __stretched__ when they are multiplied by the matrix (i.e. their direction doesn’t change, only their length, and they might also be going “backwards” if they are stretched by a negative amount). The eigenvalues reveal the degree to which the vectors are stretched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalue Decomposition vs. SVD Decomposition\n",
    "\n",
    "How does eigendecomposition differ from the SVD?\n",
    " - The basis of the SVD representation differs from the eigenvalue decomposition\n",
    "   - The basis vectors are not in general orthogonal for the eigenvalue decomposition where it is for the SVD\n",
    "   - The SVD effectively contains two basis sets.\n",
    " - All matrices have an SVD decomposition whereas not all have eigenvalue decompositions. \n",
    " \n",
    "But luckily, in our course-related problems, we usually need to decompose only a specific class of matrices that have a simple decomposition. We would get more touch with eigendecomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: display eigenvalue decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenproblems can be solved in numpy through `np.linalg.eig`. \n",
    "\n",
    "Reference: https://becominghuman.ai/deep-learning-book-notes-chapter-2-linear-algebra-for-deep-learning-af776cf52506"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([[3.0, 4.0],[1.0, 2.0]])\n",
    "diag, X = np.linalg.eig(B)\n",
    "print('Original B')\n",
    "print(B)\n",
    "print('Eigenvalues')\n",
    "print(diag)\n",
    "print('Reconstructed B')\n",
    "print(X.dot(np.diag(diag).dot(np.linalg.inv(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eig_show(x, title):\n",
    "    dest = B.dot(x)\n",
    "    plt.quiver([0, 0], [0, 0], [x[0], dest[0]], [x[1], dest[1]], angles='xy', scale_units='xy', scale=1, color=['k', 'r'])\n",
    "    plt.xlim(min(x[0], dest[0], 0) - 0.3, max(x[0], dest[0], 0) + 0.3)\n",
    "    plt.ylim(min(x[1], dest[1], 0) - 0.3, max(x[1], dest[1], 0) + 0.3)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "eig_show([1, 1], 'Non-eigenvector transformation')\n",
    "eig_show(X[:, 0], f'First eigenvector (lamba = {diag[0]:.2f})')\n",
    "eig_show(X[:, 1], f'Second eigenvector (lamba = {diag[1]:.2f})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e1c6790556d9715f0c5f336baccb730b1eb6fd7378371b19e40b5e23833f2cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
